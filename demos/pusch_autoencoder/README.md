# PUSCH Autoencoder Demo

This demo implements an end-to-end autoencoder for the 5G NR Physical Uplink Shared Channel (PUSCH). The autoencoder jointly learns the transmitter constellation and neural receiver, optimizing the entire communication link for improved performance.

## Overview

The PUSCH autoencoder learns:
- **Trainable constellation**: Optimized symbol mapping (replaces standard 16-QAM)
- **Neural detector**: Learned receiver that performs joint channel estimation and detection
- **Correction scales**: Trainable scaling factors for channel estimates, error variance, and LLRs

**Key features:**
- Ray-tracing based channel model (loaded from TFRecords)
- Multi-user MIMO support
- Two training modes: conventional and two-phase
- Imperfect CSI handling

## Training

Train the PUSCH autoencoder using one of two training modes:

### Conventional Training
Updates transmitter and receiver simultaneously:

```bash
python demos/pusch_autoencoder/training.py conventional
```

### Two-Phase Training
Alternates between receiver updates (10 steps) and transmitter updates (1 step):

```bash
python demos/pusch_autoencoder/training.py two_phase
```

**Training details:**
- Number of iterations: 5000
- Eb/N0 range: -2 dB to 10 dB
- Gradient accumulation: 16 steps
- Learning rates:
  - TX constellation: 1e-2
  - RX correction scales: 1e-2
  - RX neural network: 1e-4
- Optimizer: Adam with cosine decay schedule

**Outputs:**
- `results/PUSCH_autoencoder_weights_{mode}_training` - Final trained weights
- `results/PUSCH_autoencoder_weights_{mode}_iter_{N}` - Intermediate checkpoints (every 1000 iterations)
- `results/{mode}_training_loss.npy` - Training loss history
- `results/{mode}_training_loss.png` - Training loss plot
- `results/constellations_overlaid_{mode}.png` - Initial vs trained constellation

## Baseline Evaluation

Evaluate the traditional LMMSE baseline receiver:

```bash
python demos/pusch_autoencoder/baseline.py
```

This runs BER/BLER simulations with:
- Standard 16-QAM constellation
- LMMSE detection
- Perfect and imperfect CSI modes

**Outputs:**
- `results/baseline_results.npz` - Baseline BER/BLER results

## Inference

Run inference with the trained autoencoder:

```bash
# For conventional training
python demos/pusch_autoencoder/inference.py conventional

# For two-phase training
python demos/pusch_autoencoder/inference.py two_phase
```

**Prerequisites:**
- Trained weights at `results/PUSCH_autoencoder_weights_{mode}_training`
- Channel model TFRecords (generated by CIRManager)

**Configuration:**
- Eb/N0 range: -2 dB to 10 dB
- Max Monte Carlo iterations: 50
- Target block errors: 200

**Outputs:**
- `results/inference_results_{mode}.npz` - Autoencoder BER/BLER results

## Generating Plots

Generate comparison plots after running baseline and inference:

```bash
python demos/pusch_autoencoder/plots.py
```

**Prerequisites:**
- `results/baseline_results.npz` (baseline results)
- `results/inference_results_conventional.npz` (autoencoder results)
- `results/PUSCH_autoencoder_weights_conventional_training` (trained weights)
- `results/conventional_training_loss.npy` (training loss)

**Generated plots:**
- `results/bler_plot_bs{B}_ue{U}_ant{A}x{N}.png` - BLER comparison
- `results/training_loss.png` - Training loss curve (iterations 500-5000)
- `results/constellation_normalized.png` - Standard 16-QAM vs trained constellation
- `results/constellation_iter_{N}.png` - Constellation evolution at intermediate iterations

## Complete Workflow Example

```bash
# 1. Run baseline evaluation
python demos/pusch_autoencoder/baseline.py

# 2. Train autoencoder (conventional mode)
python demos/pusch_autoencoder/training.py conventional

# 3. Run inference with trained model
python demos/pusch_autoencoder/inference.py conventional

# 4. Generate comparison plots
python demos/pusch_autoencoder/plots.py
```

## Channel Model Setup

The demo requires ray-tracing channel impulse responses stored as TFRecords. The `CIRManager` class handles loading and preprocessing:

```python
from demos.pusch_autoencoder.src.cir_manager import CIRManager

cir_manager = CIRManager()
channel_model = cir_manager.load_from_tfrecord(group_for_mumimo=True)
```

## Running Tests

```bash
pytest demos/pusch_autoencoder/tests/ -v
```

## Configuration

Key parameters in `src/config.py`:
- `batch_size`: Training/inference batch size
- `num_ue`: Number of user equipments
- `num_bs_ant`: Number of base station antennas
- `num_ue_ant`: Number of UE antennas
- `num_time_steps`: OFDM symbols per slot

## Training Modes Comparison

| Mode | TX Updates | RX Updates | Use Case |
|------|------------|------------|----------|
| Conventional | Every iteration | Every iteration | Faster training, may be less stable |
| Two-Phase | Every 10 iterations | 10x per TX update | More stable, allows RX to adapt |
